{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d865acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"esm2_t30_150m_ur50d\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"esm2_t30_150m_ur50d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac15c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据地址： https://www.uniprot.org/uniprotkb\n",
    "\n",
    "def read_fasta_file(file_path):\n",
    "    \"\"\"\n",
    "    读取FASTA文件并返回序列信息\n",
    "    \n",
    "    参数:\n",
    "    file_path (str): FASTA文件的路径\n",
    "    \n",
    "    返回:\n",
    "    list: 包含每个序列信息的字典列表，每个字典包含'id'和'sequence'键\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    current_id = None\n",
    "    current_sequence = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                if line.startswith('>'):\n",
    "                    # 如果遇到新的序列标识符\n",
    "                    if current_id is not None:\n",
    "                        # 保存上一个序列\n",
    "                        sequences.append({\n",
    "                            'id': current_id,\n",
    "                            'sequence': ''.join(current_sequence)\n",
    "                        })\n",
    "                    # 提取新的序列标识符\n",
    "                    current_id = line[1:]\n",
    "                    current_sequence = []\n",
    "                else:\n",
    "                    # 累加序列行\n",
    "                    current_sequence.append(line)\n",
    "        \n",
    "        # 保存最后一个序列\n",
    "        if current_id is not None:\n",
    "            sequences.append({\n",
    "                'id': current_id,\n",
    "                'sequence': ''.join(current_sequence)\n",
    "            })\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 文件 '{file_path}' 未找到\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"错误: 读取文件时发生异常: {e}\")\n",
    "        return []\n",
    "\n",
    "# 使用示例\n",
    "fasta_file = \"uniprot_sprot.fasta\"\n",
    "sequences = read_fasta_file(fasta_file)\n",
    "# sequences[0]['sequence'], sequences[0]['id']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef42eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sequence = \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYN<mask>VATPRGYVLAGG\"\n",
    "inputs = tokenizer(masked_sequence, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        **inputs,\n",
    "        output_hidden_states=True,  # 返回所有层的隐藏状态\n",
    "        output_attentions=True,     # 返回注意力权重\n",
    "    )\n",
    "logits = outputs.logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]  # 找到<mask>的位置\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(-1)  # 取分数最高的氨基酸\n",
    "predicted_aa = tokenizer.decode(predicted_token_id)  # 解码为氨基酸符号\n",
    "print(f\"Predicted amino acid: {predicted_aa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fed419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(seq):\n",
    "    inputs = tokenizer(seq, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    return outputs.hidden_states[-1][0, 0, :]  # 取[CLS]嵌入\n",
    "\n",
    "emb1 = get_embedding(seq1)\n",
    "emb2 = get_embedding(seq2)\n",
    "similarity = torch.cosine_similarity(emb1, emb2, dim=0)\n",
    "print(f\"Embedding Similarity: {similarity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 蛋白质序列的embedding提取 (Feature Extraction)\n",
    "inputs = tokenizer(masked_sequence, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        **inputs,\n",
    "        output_hidden_states=True,  # 返回所有层的隐藏状态\n",
    "        output_attentions=True,     # 返回注意力权重\n",
    "    )\n",
    "\n",
    "last_hidden_states = outputs.hidden_states[-1]  \n",
    "print(last_hidden_states.shape)\n",
    "cls_embedding = last_hidden_states[0, 0, :] #最后一层的输出\n",
    "# print(outputs)\n",
    "# print(last_hidden_states)\n",
    "print(cls_embedding.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
